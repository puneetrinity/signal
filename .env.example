# ============================================
# Clerk Authentication (REQUIRED)
# ============================================
# Get these from Clerk Dashboard > API Keys
# https://dashboard.clerk.com
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=""
CLERK_SECRET_KEY=""

# Clerk redirect URLs (optional, defaults work for most cases)
NEXT_PUBLIC_CLERK_SIGN_IN_URL="/sign-in"
NEXT_PUBLIC_CLERK_SIGN_UP_URL="/sign-up"
NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL="/"
NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL="/"

# ============================================
# Database (Supabase PostgreSQL)
# Get your connection string from Supabase > Project Settings > Database > Connection Pooling
DATABASE_URL="postgresql://postgres.[PROJECT-REF]:[YOUR-PASSWORD]@aws-1-us-east-1.pooler.supabase.com:5432/postgres"

# Optional but recommended: direct (non-pooler) connection for migrations / introspection
DIRECT_URL="postgresql://postgres.[PROJECT-REF]:[YOUR-PASSWORD]@db.[PROJECT-REF].supabase.co:5432/postgres"

# Supabase (optional - for client-side requests)
# Get these from Supabase > Project Settings > API
NEXT_PUBLIC_SUPABASE_URL="https://[PROJECT-REF].supabase.co"
NEXT_PUBLIC_SUPABASE_ANON_KEY="your-anon-key-here"

# Bright Data API
# Get your API token from Bright Data > Settings > API Tokens
BRIGHTDATA_API_TOKEN="your-brightdata-api-token-here"

# Google AI (Gemini)
# Get your API key from Google AI Studio: https://makersuite.google.com/app/apikey
GOOGLE_GENERATIVE_AI_API_KEY="your-google-ai-api-key-here"

# Next.js
# Your application URL (use http://localhost:3000 for local development)
NEXT_PUBLIC_APP_URL="http://localhost:3000"

# Optional: Analytics
# PostHog API key (leave empty if not using analytics)
NEXT_PUBLIC_POSTHOG_KEY=""

# Redis (optional, for hot cache). Provide either:
# - REDIS_URL, or
# - REDIS_HOST + REDIS_PORT (+ optional REDIS_PASSWORD)
REDIS_URL="rediss://:your_password_here@redis-xxxxx.c123.us-east-1-1.ec2.cloud.redislabs.com:6379"
REDIS_TLS_ENABLED="true"

REDIS_HOST="redis-xxxxx.c123.us-east-1-1.ec2.cloud.redislabs.com"
REDIS_PORT="xxxxx"
REDIS_PASSWORD="your_password_here"

# Rate limiting backend (recommended: redis on multi-instance deployments)
# Options: memory, redis
# Default: memory (works for single instance only)
RATE_LIMIT_BACKEND="memory"

# Optional: namespace prefix for rate limiting keys in Redis
RATE_LIMIT_PREFIX="ratelimit"

# Image proxy allowlist (optional). Comma-separated hosts and/or suffixes (prefix with a dot).
# Defaults to: .licdn.com
IMAGE_PROXY_ALLOWED_HOSTS=".licdn.com"

# ============================================
# v2 Provider Configuration (Feature Flags)
# ============================================

# Search Provider Selection
# Options: brightdata (default), searxng, brave, serper
# Default is 'brightdata' for backward compatibility with v1
SEARCH_PROVIDER="brightdata"

# Optional: Fallback search provider when primary returns no results
# Set to enable SearXNG primary with Brave fallback:
#   SEARCH_PROVIDER="searxng"
#   SEARCH_FALLBACK_PROVIDER="brave"
SEARCH_FALLBACK_PROVIDER=""

# SearXNG Configuration (if SEARCH_PROVIDER=searxng)
# Self-hosted metasearch instance URL
SEARXNG_URL="https://searxng-railway-production-9236.up.railway.app"
SEARXNG_TIMEOUT="10000"

# Serper.dev (Google SERP) Configuration (if SEARCH_PROVIDER=serper or ENRICHMENT_SEARCH_PROVIDER=serper)
# Get your API key from: https://serper.dev/
SERPER_API_KEY=""
# Optional: override endpoint (default: https://google.serper.dev/search)
SERPER_URL="https://google.serper.dev/search"
SERPER_TIMEOUT="8000"
# Pagination caps
SERPER_MAX_PAGES="3"
SERPER_NUM_PER_PAGE="20"

# Brave Search API (if SEARCH_PROVIDER=brave or SEARCH_FALLBACK_PROVIDER=brave)
# Get your API key from: https://brave.com/search/api/
# Pricing: Free (2K/month), Base ($5/1000), Pro ($9/1000)
BRAVE_API_KEY=""
BRAVE_TIMEOUT="8000"
# Brave pagination cap (Brave max is 20 results per request; pagination uses offset)
# Default: 3 pages (up to 60 results max per query)
BRAVE_MAX_PAGES="3"

# Provider concurrency smoothing (important: don't rely on "300 req/sec" marketing numbers)
# Limits concurrent outbound requests per provider process.
# Used by Serper and Brave providers (and as a fallback by enrichment if set).
SEARCH_PROVIDER_CONCURRENCY="2"
ENRICHMENT_PROVIDER_CONCURRENCY="2"

# Parser Provider Selection
# Options: gemini (default), groq
# Default is 'gemini' for backward compatibility
PARSER_PROVIDER="gemini"

# Groq Configuration (if PARSER_PROVIDER=groq)
# Get your API key from: https://console.groq.com/
# NOTE: Requires npm install @ai-sdk/groq
GROQ_API_KEY=""
# Optional: Override default model (default: meta-llama/llama-4-scout-17b-16e-instruct)
# Must support structured outputs: https://console.groq.com/docs/structured-outputs#supported-models
# Options: meta-llama/llama-4-scout-17b-16e-instruct, meta-llama/llama-4-maverick-17b-128e-instruct, openai/gpt-oss-20b
GROQ_MODEL=""

# v2 Feature Flags
# Enable new discovery mode (leads/candidates instead of profiles)
USE_NEW_DISCOVERY="false"

# Recommended provider setup (v2):
# - Serper primary + Brave fallback for both discovery and enrichment:
#   SEARCH_PROVIDER="serper"
#   SEARCH_FALLBACK_PROVIDER="brave"
#   ENRICHMENT_SEARCH_PROVIDER="serper"
#   ENRICHMENT_SEARCH_FALLBACK_PROVIDER="brave"

# ============================================
# v1 Deprecation / Compliance Flags
# ============================================

# Disable v1 endpoints that scrape or return full LinkedIn profile data.
# Recommended: set to "true" in production once you migrate to v2.
# Notes:
# - When USE_NEW_DISCOVERY="true", v1 scraping endpoints should be treated as deprecated anyway.
DISABLE_V1_SCRAPING="false"

# ============================================
# v2 Enrichment Configuration
# ============================================

# GitHub API Token (for identity discovery)
# Get a Personal Access Token from: https://github.com/settings/tokens
# Recommended scopes: public_repo (or just use no scopes for read-only public access)
# Rate limits: 60 req/hr unauthenticated, 5000 req/hr with token
GITHUB_TOKEN=""

# Enrichment Search Provider Configuration
# Separate from main search to allow different strategies
# Enrichment order: GitHub API → Primary (SERP) → Fallback (optional)
#
# Options: serper (recommended), brave, searxng, brightdata
# Default is 'brave' for backwards compatibility (set to 'serper' for better Google-style coverage)
ENRICHMENT_SEARCH_PROVIDER="brave"

# Fallback provider when primary returns too few results or fails
# Options: brave, serper, searxng, brightdata, none
# Recommended: "none" when SearXNG is blocked/rate-limited by CAPTCHA
# Note: If unset, fallback defaults to "searxng"
ENRICHMENT_SEARCH_FALLBACK_PROVIDER="none"

# Minimum results from primary before trying fallback
# If primary returns fewer results, fallback will be tried
MIN_RESULTS_BEFORE_FALLBACK="2"

# ============================================
# v2 LangGraph Async Enrichment (Optional)
# ============================================

# Enable LangGraph-based async enrichment
# When true: POST /api/v2/enrich/async creates a job and returns immediately
# When false: Enrichment runs synchronously (default)
USE_LANGGRAPH_ENRICHMENT="false"

# Enable Postgres checkpointer for graph state persistence and resumability
# Requires DIRECT_URL or DATABASE_URL to be set
# When true: Graph state is checkpointed to Postgres, allowing job resumption
# When false: Graph runs without checkpointing (default)
USE_LANGGRAPH_CHECKPOINTER="false"

# Groq model for recruiter summary generation (optional)
# Defaults to GROQ_MODEL, otherwise: meta-llama/llama-4-scout-17b-16e-instruct
ENRICHMENT_SUMMARY_MODEL=""

# Worker concurrency (number of parallel jobs)
# Only relevant when running the worker: npm run worker:enrichment
ENRICHMENT_WORKER_CONCURRENCY="3"

# ============================================
# v2 Enrichment Budget & Rate Limiting
# ============================================

# Minimum confidence score to persist an identity match
# Default: 0.35 (requires name match + secondary signal)
# Range: 0.0-1.0. Lower = more matches but more noise
ENRICHMENT_MIN_CONFIDENCE="0.35"

# Maximum platforms to query in parallel (cost/rate limit control)
# Default: 3
ENRICHMENT_MAX_PARALLEL_PLATFORMS="3"

# Maximum total platforms to query per enrichment
# Default: 5
ENRICHMENT_MAX_PLATFORMS="5"

# Maximum total queries across all platforms
# Default: 30
ENRICHMENT_MAX_QUERIES="30"

# Phase B3: Query normalization (diacritics/punctuation) for name-mode queries
# Default: true (only triggers when the initial query returns 0 matched results)
ENRICHMENT_ENABLE_QUERY_NORMALIZATION="true"

# ============================================
# v2 Compliance & Privacy Controls
# ============================================

# Enable gathering commit email evidence pointers from GitHub
# Default: false (disabled for compliance)
# When true: Stores pointers to commits (URLs, not emails)
# Actual email extraction requires DISABLE_EMAIL_EXTRACTION=false
ENABLE_COMMIT_EMAIL_EVIDENCE="false"

# Completely disable email extraction from commits
# Default: false (extraction allowed when requested)
# When true: extractEmailFromCommit() always returns null
DISABLE_EMAIL_EXTRACTION="false"

# ============================================
# v2 Platform Reliability Controls
# ============================================

# Skip known-unreliable platforms (crunchbase, angellist)
# Default: false (unreliable platforms are deprioritized but still queried)
# When true: Unreliable platforms are skipped entirely
SKIP_UNRELIABLE_PLATFORMS="false"

# Additional platforms to mark as unreliable (comma-separated)
# These will be deprioritized or skipped based on SKIP_UNRELIABLE_PLATFORMS
# Example: "twitter,medium"
UNRELIABLE_PLATFORMS=""

# Redis URL for BullMQ job queue (required if USE_LANGGRAPH_ENRICHMENT=true)
# Reuses REDIS_URL if set, otherwise defaults to localhost
# REDIS_URL is already defined above - BullMQ will use it

# ============================================
# v2 Authentication (REQUIRED for production)
# ============================================

# API Keys for machine-to-machine auth (comma-separated)
# Generate secure random keys for production
# Example: openssl rand -hex 32
API_KEYS=""

# Auth enforcement (defaults based on NODE_ENV)
# In production: auth enforced unless DISABLE_AUTH=true
# In development: auth not enforced unless ENFORCE_AUTH=true
ENFORCE_AUTH="false"
# DISABLE_AUTH="false"  # Only set in production to temporarily disable auth
